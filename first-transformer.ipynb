{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1) Importations","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# 2) Preprocessing input et output embedding","metadata":{}},{"cell_type":"code","source":"input_embedding = [[\"Salut\", \"comment\", \"ca\", \"va\", \"?\"]] #1 batch de 1 sequence\noutput_embedding = [[\"<START>\", \"Hi\", \"how\", \"are\", \"you\", \"?\"]]","metadata":{"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def get_vocabulary(sequences):\n    token_to_info = {}\n    for sequence in sequences:\n        for word in sequence:\n            if word not in token_to_info:                #Pas de doublons dans les tokens\n                token_to_info[word] = len(token_to_info) #On donne un ID au token, qui sera la longueur de la liste de token\n    return token_to_info","metadata":{"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"input_voc = get_vocabulary(input_embedding)\noutput_voc = get_vocabulary(output_embedding)\nprint(input_voc)\nprint(output_voc)","metadata":{"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"{'Salut': 0, 'comment': 1, 'ca': 2, 'va': 3, '?': 4}\n{'<START>': 0, 'Hi': 1, 'how': 2, 'are': 3, 'you': 4, '?': 5}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### --> On ajoute les tokens spécifiques","metadata":{}},{"cell_type":"code","source":"input_voc[\"<START>\"] = len(input_voc)\ninput_voc[\"<END>\"] = len(input_voc)\ninput_voc[\"<PAD >\"] = len(input_voc) #Le padding est utile pr remplir les sequences n'étant pas de même taille que d'autres\n\noutput_voc[\"<END>\"] = len(input_voc) #Attention à ne pas ajouter <START> ici si déjà fait dans le output_embedding !!!\noutput_voc[\"<PAD >\"] = len(input_voc)","metadata":{"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### --> Transformation des mots en int pour notre modèle","metadata":{}},{"cell_type":"code","source":"def sequences_to_int(sequences, voc):\n    for sequence in sequences:\n        for index, word in enumerate(sequence):\n            sequence[index] = voc[word]\n    return np.array(sequences)","metadata":{"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"input_seq = sequences_to_int(input_embedding, input_voc)\noutput_seq = sequences_to_int(output_embedding, output_voc)\nprint(input_seq)\nprint(output_seq)","metadata":{"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"[[0 1 2 3 4]]\n[[0 1 2 3 4 5]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 3) Layers","metadata":{}},{"cell_type":"markdown","source":"### --> Input embedding layer","metadata":{}},{"cell_type":"code","source":"class InputEmbedding(tf.keras.layers.Layer):\n    def __init__(self, nb_token, **kwargs):\n        self.nb_token = nb_token\n        super(**kwargs).__init__()\n        \n    def build(self, input_shape):\n        self.word_embedding = tf.keras.layers.Embedding(self.nb_token, 256)\n        super().build(input_shape)\n        \n    def call(self, x):\n        embed = self.word_embedding(x)\n        return embed","metadata":{"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### --> Scaled Dot-Product Attention","metadata":{}},{"cell_type":"code","source":"class ScaledDotProductAttention(tf.keras.layers.Layer):\n    def __init__(self, **kwargs):\n        super(**kwargs).__init__()\n        \n    def build(self, input_shape):\n        self.query_layer = tf.keras.layers.Dense(256)\n        self.key_layer   = tf.keras.layers.Dense(256)\n        self.value_layer = tf.keras.layers.Dense(256)\n        super().build(input_shape)\n        \n    def call(self, x):\n        Q = self.query_layer(x)\n        K = self.key_layer(x)\n        V = self.value_layer(x)\n        QK = tf.matmul(Q, K, transpose_b=True)\n        QK = QK / tf.math.sqrt(256.0)           #Normalise  les valeurs\n        softmax_QK = tf.nn.softmax(QK, axis=-1) #Attention pour chaque mots de la sequence\n        attention = tf.matmul(softmax_QK, V)    #Applique notre attention aux V\n        return attention","metadata":{"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def test_ScaledDotProductAttention():\n    layer_input = tf.keras.Input(shape=(5)) #Taille sequence : 5 (On peut gerer les autres tailles avec de PAD)\n    input_embedding = InputEmbedding(nb_token=5)(layer_input)\n    attention = ScaledDotProductAttention()(input_embedding)\n    model = tf.keras.Model(layer_input, attention)\n    model.summary()\n    return model\n    \nmodel_test = test_ScaledDotProductAttention()","metadata":{"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 5)]               0         \n_________________________________________________________________\ninput_embedding (InputEmbedd (None, 5, 256)            1280      \n_________________________________________________________________\nscaled_dot_product_attention (None, 5, 256)            197376    \n=================================================================\nTotal params: 198,656\nTrainable params: 198,656\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# --> Multi Head Attention","metadata":{}},{"cell_type":"code","source":"class MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(self, dim=256, nb_head=8, **kwargs): #dim doit être divisible par nb_head\n        self.head_dim = dim//nb_head\n        self.nb_head = nb_head\n        print(self.head_dim)\n        super(**kwargs).__init__()\n        \n    def build(self, input_shape):\n        self.query_layer = tf.keras.layers.Dense(256)\n        self.key_layer   = tf.keras.layers.Dense(256)\n        self.value_layer = tf.keras.layers.Dense(256)\n        super().build(input_shape)\n        \n    def call(self, x):\n        Q = self.query_layer(x)\n        K = self.key_layer(x)\n        V = self.value_layer(x)\n        \n        batch_size = tf.shape(Q)[0]\n        seq_len = tf.shape(Q)[1]\n        Q = tf.reshape(Q, [batch_size, seq_len, self.nb_head, self.head_dim])\n        K = tf.reshape(K, [batch_size, seq_len, self.nb_head, self.head_dim])\n        V = tf.reshape(V, [batch_size, seq_len, self.nb_head, self.head_dim])\n        \n        Q = tf.transpose(Q, [0, 2, 1, 3])\n        K = tf.transpose(Q, [0, 2, 1, 3])\n        V = tf.transpose(Q, [0, 2, 1, 3])\n        \n        return attention","metadata":{"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"### --> Encoder layer","metadata":{}},{"cell_type":"code","source":"class EncoderLayer(tf.keras.layers.Layer):\n    def __init__(self, **kwargs):\n        super(**kwargs).__init__()\n        \n    def build(self, input_shape):\n        self.scaled_dot_product_attention = ScaledDotProductAttention()\n        self.norm = tf.keras.layers.LayerNormalization()\n        self.feed_forward = tf.keras.layers.Dense(256)\n        super().build(input_shape)\n        \n    def call(self, x):\n        attention = self.scaled_dot_product_attention(x) #Multi-Head Attention\n        post_attention = self.norm(attention + x)        #Add & Norm\n        feed_forward = self.feed_forward(post_attention) #Feed Forward\n        enc_output = self.norm(x + post_attention)       #2nd Add & Norm\n        print(post_attention.shape)\n        return enc_output","metadata":{"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def test_EncoderLayer():\n    layer_input = tf.keras.Input(shape=(5)) #Taille sequence : 5 (On peut gerer les autres tailles avec de PAD)\n    input_embedding = InputEmbedding(nb_token=5)(layer_input)\n    encoder_output = EncoderLayer()(input_embedding)\n    model = tf.keras.Model(layer_input, encoder_output)\n    model.summary()\n    return model\n    \nmodel_test = test_EncoderLayer()","metadata":{"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"(None, 5, 256)\nModel: \"model_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         [(None, 5)]               0         \n_________________________________________________________________\ninput_embedding_1 (InputEmbe (None, 5, 256)            1280      \n_________________________________________________________________\nencoder_layer (EncoderLayer) (None, 5, 256)            263680    \n=================================================================\nTotal params: 264,960\nTrainable params: 264,960\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}