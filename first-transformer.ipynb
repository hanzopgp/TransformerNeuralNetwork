{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1) Importations","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","metadata":{"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# 2) Preprocessing input et output embedding","metadata":{}},{"cell_type":"code","source":"input_embedding = [[\"Salut\", \"comment\", \"ca\", \"va\", \"?\"]] #1 batch de 1 sequence\noutput_embedding = [[\"<START>\", \"Hi\", \"how\", \"are\", \"you\", \"?\"]]\n\nNB_ENCODER = 6\nNB_DECODER = 6","metadata":{"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def get_vocabulary(sequences):\n    token_to_info = {}\n    for sequence in sequences:\n        for word in sequence:\n            if word not in token_to_info:                #Pas de doublons dans les tokens\n                token_to_info[word] = len(token_to_info) #On donne un ID au token, qui sera la longueur de la liste de token\n    return token_to_info","metadata":{"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"input_voc = get_vocabulary(input_embedding)\noutput_voc = get_vocabulary(output_embedding)\nprint(input_voc)\nprint(output_voc)","metadata":{"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"{'Salut': 0, 'comment': 1, 'ca': 2, 'va': 3, '?': 4}\n{'<START>': 0, 'Hi': 1, 'how': 2, 'are': 3, 'you': 4, '?': 5}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### --> On ajoute les tokens spécifiques","metadata":{}},{"cell_type":"code","source":"input_voc[\"<START>\"] = len(input_voc)\ninput_voc[\"<END>\"] = len(input_voc)\ninput_voc[\"<PAD >\"] = len(input_voc) #Le padding est utile pr remplir les sequences n'étant pas de même taille que d'autres\n\noutput_voc[\"<END>\"] = len(input_voc) #Attention à ne pas ajouter <START> ici si déjà fait dans le output_embedding !!!\noutput_voc[\"<PAD >\"] = len(input_voc)\n\nprint(input_voc)\nprint(output_voc)\n\nNB_TOKEN = len(input_embedding[0])\nSEQ_LEN = len(input_embedding[0])\n\nNB_TOKEN_OUT = len(output_embedding[0])\nSEQ_LEN_OUT = len(output_embedding[0])","metadata":{"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"{'Salut': 0, 'comment': 1, 'ca': 2, 'va': 3, '?': 4, '<START>': 5, '<END>': 6, '<PAD >': 7}\n{'<START>': 0, 'Hi': 1, 'how': 2, 'are': 3, 'you': 4, '?': 5, '<END>': 8, '<PAD >': 8}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### --> Transformation des mots en int pour notre modèle","metadata":{}},{"cell_type":"code","source":"def sequences_to_int(sequences, voc):\n    for sequence in sequences:\n        for index, word in enumerate(sequence):\n            sequence[index] = voc[word]\n    return np.array(sequences)","metadata":{"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"input_seq = sequences_to_int(input_embedding, input_voc)\noutput_seq = sequences_to_int(output_embedding, output_voc)\nprint(input_seq)\nprint(output_seq)","metadata":{"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"[[0 1 2 3 4]]\n[[0 1 2 3 4 5]]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 3) Layers","metadata":{}},{"cell_type":"markdown","source":"### --> Input embedding layer","metadata":{}},{"cell_type":"code","source":"class Embedding(tf.keras.layers.Layer):\n    def __init__(self, nb_token, **kwargs):\n        self.nb_token = nb_token\n        super(**kwargs).__init__()\n        \n    def build(self, input_shape):\n        self.word_embedding = tf.keras.layers.Embedding(self.nb_token, 256)\n        super().build(input_shape)\n        \n    def call(self, x):\n        embed = self.word_embedding(x)\n        return embed","metadata":{"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"### --> Scaled Dot-Product Attention","metadata":{}},{"cell_type":"code","source":"class ScaledDotProductAttention(tf.keras.layers.Layer):\n    def __init__(self, **kwargs):\n        super(**kwargs).__init__()\n        \n    def build(self, input_shape):\n        self.query_layer = tf.keras.layers.Dense(256)\n        self.key_layer   = tf.keras.layers.Dense(256)\n        self.value_layer = tf.keras.layers.Dense(256)\n        super().build(input_shape)\n        \n    def call(self, x):\n        Q = self.query_layer(x)\n        K = self.key_layer(x)\n        V = self.value_layer(x)\n        QK = tf.matmul(Q, K, transpose_b=True)\n        QK = QK / tf.math.sqrt(256.0)           #Normalise  les valeurs\n        softmax_QK = tf.nn.softmax(QK, axis=-1) #Attention pour chaque mots de la sequence\n        attention = tf.matmul(softmax_QK, V)    #Applique notre attention aux V\n        return attention","metadata":{"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def test_ScaledDotProductAttention():\n    layer_input = tf.keras.Input(shape=(SEQ_LEN)) #Taille sequence : 5 (On peut gerer les autres tailles avec de PAD)\n    embedding = Embedding(nb_token=NB_TOKEN)(layer_input)\n    attention = ScaledDotProductAttention()(embedding)\n    model = tf.keras.Model(layer_input, attention)\n    model.summary()\n    return model\n    \nmodel_test = test_ScaledDotProductAttention()\nout = model_test(input_seq)\nprint(out.shape)","metadata":{"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 5)]               0         \n_________________________________________________________________\nembedding (Embedding)        (None, 5, 256)            1280      \n_________________________________________________________________\nscaled_dot_product_attention (None, 5, 256)            197376    \n=================================================================\nTotal params: 198,656\nTrainable params: 198,656\nNon-trainable params: 0\n_________________________________________________________________\n(1, 5, 256)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### --> Multi Head Attention / Masket Multi Head Attention","metadata":{}},{"cell_type":"code","source":"class MultiHeadAttention(tf.keras.layers.Layer):\n    def __init__(self, dim=256, nb_head=8, **kwargs): #dim doit être divisible par nb_head\n        self.dim = dim\n        self.head_dim = dim//nb_head\n        self.nb_head = nb_head\n        super(**kwargs).__init__()\n        \n    def build(self, input_shape):\n        self.query_layer = tf.keras.layers.Dense(256)\n        self.key_layer   = tf.keras.layers.Dense(256)\n        self.value_layer = tf.keras.layers.Dense(256)\n        self.output_layer = tf.keras.layers.Dense(256)\n        super().build(input_shape)\n        \n    def mask_softmax(self, x, mask):\n        x_exp = tf.math.exp(x)\n        x_exp_masked = x_exp * mask\n        x_exp_sum = tf.reduce_sum(x_exp_masked, axis=-1)\n        x_exp_sum = tf.expand_dims(x_exp_sum, axis=-1)\n        softmax = x_exp_masked / x_exp_sum\n        return softmax\n        \n    def call(self, x, mask=None):\n        in_Q, in_K, in_V = x\n        \n        Q = self.query_layer(in_Q)\n        K = self.key_layer(in_K)\n        V = self.value_layer(in_V)\n        \n        batch_size = tf.shape(Q)[0]\n        Q_seq_len = tf.shape(Q)[1]\n        K_seq_len = tf.shape(K)[1]\n        V_seq_len = tf.shape(V)[1]\n        \n        Q = tf.reshape(Q, [batch_size, Q_seq_len, self.nb_head, self.head_dim])\n        K = tf.reshape(K, [batch_size, K_seq_len, self.nb_head, self.head_dim])\n        V = tf.reshape(V, [batch_size, V_seq_len, self.nb_head, self.head_dim])\n          \n        Q = tf.transpose(Q, [0, 2, 1, 3])\n        K = tf.transpose(K, [0, 2, 1, 3])\n        V = tf.transpose(V, [0, 2, 1, 3])\n\n        Q = tf.reshape(Q, [batch_size * self.nb_head, Q_seq_len, self.head_dim])\n        K = tf.reshape(K, [batch_size * self.nb_head, K_seq_len, self.head_dim])\n        V = tf.reshape(V, [batch_size * self.nb_head, V_seq_len, self.head_dim])\n        \n        #Scaled dot product attention\n        QK = tf.matmul(Q, K, transpose_b=True)\n        QK = QK / tf.math.sqrt(float(self.dim)) #Normalise  les valeurs\n        \n        #Mask\n        #Ici on veut éviter de donner les réponses au réseau dans le decoder\n        #On veut donc que l'attention ne porte que sur les éléments précédents ainsi que de la self-attention\n        #[1, 0, 0]\n        #[1, 1, 0]\n        #[1, 1, 1]\n        #Voila ce qu'on veut obtenir\n        if mask is not None:\n            QK = QK * mask\n            softmax_QK = self.mask_softmax(QK, mask) #Softmax custom pour éviter les problèmes liés aux valuers à 0 à cause du mask\n        else:\n            softmax_QK = tf.nn.softmax(QK, axis=-1)  #Attention pour chaque mots de la sequence\n        \n        attention = tf.matmul(softmax_QK, V)         #Applique notre attention aux V\n        \n        #Concatenation des scaled dot product attention\n        attention = tf.reshape(attention, [batch_size, self.nb_head, Q_seq_len, self.head_dim])\n        attention = tf.transpose(attention, [0, 2, 1, 3])\n        attention = tf.reshape(attention, [batch_size, Q_seq_len, self.nb_head * self.head_dim])\n        \n        out_attention = self.output_layer(attention)\n        \n        return out_attention","metadata":{"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def test_MultiHeadAttention():\n    layer_input = tf.keras.Input(shape=(SEQ_LEN_OUT))\n    embedding = Embedding(nb_token=NB_TOKEN_OUT)(layer_input)\n    attention = MultiHeadAttention()((embedding, embedding, embedding))\n    model = tf.keras.Model(layer_input, attention)\n    model.summary()\n    return model\n    \nmodel_test = test_MultiHeadAttention()\nout = model_test(output_seq)\nprint(out.shape)","metadata":{"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Model: \"model_1\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_2 (InputLayer)            [(None, 6)]          0                                            \n__________________________________________________________________________________________________\nembedding_1 (Embedding)         (None, 6, 256)       1536        input_2[0][0]                    \n__________________________________________________________________________________________________\nmulti_head_attention (MultiHead (None, None, 256)    263168      embedding_1[0][0]                \n                                                                 embedding_1[0][0]                \n                                                                 embedding_1[0][0]                \n==================================================================================================\nTotal params: 264,704\nTrainable params: 264,704\nNon-trainable params: 0\n__________________________________________________________________________________________________\n(1, 6, 256)\n","output_type":"stream"}]},{"cell_type":"code","source":"def test_MaskedMultiHeadAttention():\n    layer_input = tf.keras.Input(shape=(SEQ_LEN_OUT)) #Taille sequence : 6 (On peut gerer les autres tailles avec de PAD)\n    embedding = Embedding(nb_token=NB_TOKEN_OUT)(layer_input)\n    \n    #Mask\n    mask = tf.sequence_mask(tf.range(SEQ_LEN_OUT) + 1, SEQ_LEN_OUT)\n    mask = tf.cast(mask, tf.float32)\n    mask = tf.expand_dims(mask, axis=0)\n        \n    attention = MultiHeadAttention()((embedding, embedding, embedding), mask=None)\n    model = tf.keras.Model(layer_input, attention)\n    model.summary()\n    \n    return model\n    \nmodel_test = test_MaskedMultiHeadAttention()\nout = model_test(output_seq)\nprint(out.shape)","metadata":{"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Model: \"model_2\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_3 (InputLayer)            [(None, 6)]          0                                            \n__________________________________________________________________________________________________\nembedding_2 (Embedding)         (None, 6, 256)       1536        input_3[0][0]                    \n__________________________________________________________________________________________________\nmulti_head_attention_1 (MultiHe (None, None, 256)    263168      embedding_2[0][0]                \n                                                                 embedding_2[0][0]                \n                                                                 embedding_2[0][0]                \n==================================================================================================\nTotal params: 264,704\nTrainable params: 264,704\nNon-trainable params: 0\n__________________________________________________________________________________________________\n(1, 6, 256)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### --> Encoder layer","metadata":{}},{"cell_type":"code","source":"class EncoderLayer(tf.keras.layers.Layer):\n    def __init__(self, **kwargs):\n        super(**kwargs).__init__()\n        \n    def build(self, input_shape):\n        self.multi_head_attention = MultiHeadAttention()\n        self.norm = tf.keras.layers.LayerNormalization()\n        self.feed_forward = tf.keras.layers.Dense(256)\n        super().build(input_shape)\n        \n    def call(self, x):\n        attention = self.multi_head_attention((x, x, x)) #Multi Head Attention, x x x because self attention\n        post_attention = self.norm(attention + x)        #Add & Norm\n        feed_forward = self.feed_forward(post_attention) #Feed Forward\n        enc_output = self.norm(x + post_attention)       #2nd Add & Norm\n        return enc_output","metadata":{"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def test_EncoderLayer():\n    layer_input = tf.keras.Input(shape=(SEQ_LEN)) #Taille sequence : 5 (On peut gerer les autres tailles avec de PAD)\n    input_embedding = Embedding(nb_token=NB_TOKEN)(layer_input)\n    encoder_output = EncoderLayer()(input_embedding)\n    model = tf.keras.Model(layer_input, encoder_output)\n    model.summary()\n    return model\n    \nmodel_test = test_EncoderLayer()","metadata":{"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Model: \"model_3\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_4 (InputLayer)         [(None, 5)]               0         \n_________________________________________________________________\nembedding_3 (Embedding)      (None, 5, 256)            1280      \n_________________________________________________________________\nencoder_layer (EncoderLayer) (None, 5, 256)            329472    \n=================================================================\nTotal params: 330,752\nTrainable params: 330,752\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### --> Decoder Layer","metadata":{}},{"cell_type":"code","source":"class DecoderLayer(tf.keras.layers.Layer):\n    def __init__(self, **kwargs):\n        super(**kwargs).__init__()\n        \n    def build(self, input_shape):\n        self.multi_head_self_attention = MultiHeadAttention()\n        self.norm = tf.keras.layers.LayerNormalization()\n        self.output_layer = tf.keras.layers.Dense(256)\n        super().build(input_shape)\n        \n    def call(self, x):\n        encoder_output, output_embedding, mask = x\n        self_attention = self.multi_head_self_attention((output_embedding, output_embedding, output_embedding), mask=mask) #Masked ulti Head Self Attention\n        post_self_attention = self.norm(output_embedding + self_attention)                                                 #Skip\n        encoder_attention = self.multi_head_self_attention((post_self_attention, encoder_output, encoder_output))          #Multi Head Attention\n        post_encoder_attention = self.norm(encoder_attention + post_self_attention)                                        #Skip\n        output = self.output_layer(post_self_attention)                                                                    #Output\n        decoder_output = self.norm(output + post_encoder_attention)                                                        #Skip\n        return decoder_output","metadata":{"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def test_DecoderLayer():\n    layer_input = tf.keras.Input(shape=(SEQ_LEN)) #Taille sequence : 5 (On peut gerer les autres tailles avec de PAD)\n    input_embedding = Embedding(nb_token=NB_TOKEN)(layer_input)\n    encoder_output = EncoderLayer()(input_embedding)\n    model = tf.keras.Model(layer_input, encoder_output)\n    model.summary()\n    return model\n    \nmodel_test = test_EncoderLayer()","metadata":{"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Model: \"model_4\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_5 (InputLayer)         [(None, 5)]               0         \n_________________________________________________________________\nembedding_4 (Embedding)      (None, 5, 256)            1280      \n_________________________________________________________________\nencoder_layer_1 (EncoderLaye (None, 5, 256)            329472    \n=================================================================\nTotal params: 330,752\nTrainable params: 330,752\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 4) Encoder","metadata":{}},{"cell_type":"code","source":"class Encoder(tf.keras.layers.Layer):\n    def __init__(self, nb_encoder, **kwargs):\n        self.nb_encoder = nb_encoder\n        super(**kwargs).__init__()\n        \n    def build(self, input_shape):\n        self.encoder_layers = []\n        for n in range(self.nb_encoder):\n            self.encoder_layers.append(EncoderLayer())\n        super().build(input_shape)\n        \n    def call(self, x):\n        for encoder_layer in self.encoder_layers:\n            x = encoder_layer(x)\n        return x","metadata":{"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"def test_Encoder():\n    layer_input = tf.keras.Input(shape=(SEQ_LEN)) #Taille sequence : 5 (On peut gerer les autres tailles avec de PAD)\n    input_embedding = Embedding(nb_token=NB_TOKEN)(layer_input)\n    encoder_output = Encoder(NB_ENCODER)(input_embedding)\n    model = tf.keras.Model(layer_input, encoder_output)\n    model.summary()\n    return model\n    \nmodel_test = test_Encoder()\nout = model_test(input_seq)\nprint(out.shape)","metadata":{"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Model: \"model_5\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_6 (InputLayer)         [(None, 5)]               0         \n_________________________________________________________________\nembedding_5 (Embedding)      (None, 5, 256)            1280      \n_________________________________________________________________\nencoder (Encoder)            (None, 5, 256)            1976832   \n=================================================================\nTotal params: 1,978,112\nTrainable params: 1,978,112\nNon-trainable params: 0\n_________________________________________________________________\n(1, 5, 256)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 5) Decoder","metadata":{}},{"cell_type":"code","source":"class Decoder(tf.keras.layers.Layer):\n    def __init__(self, nb_decoder, **kwargs):\n        self.nb_encoder = nb_decoder\n        super(**kwargs).__init__()\n        \n    def build(self, input_shape):\n        self.decoder_layers = []\n        for n in range(self.nb_encoder):\n            self.decoder_layers.append(DecoderLayer())\n        super().build(input_shape)\n        \n    def call(self, x):\n        encoder_output, output_embedding, mask = x\n        decoder_output = output_embedding\n        for decoder_layer in self.decoder_layers:\n            decoder_output = decoder_layer((encoder_output, decoder_output, mask))\n        return decoder_output","metadata":{"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def build_transformer():\n    input_token  = tf.keras.Input(shape=(SEQ_LEN)) #Taille sequence : 5 (On peut gerer les autres tailles avec de PAD)\n    output_token = tf.keras.Input(shape=(SEQ_LEN_OUT)) #Taille sequence : 6 \n\n    #Positional encoding\n    input_pos  = Embedding(nb_token=NB_TOKEN)(tf.range(SEQ_LEN))\n    output_pos = Embedding(nb_token=NB_TOKEN_OUT)(tf.range(SEQ_LEN_OUT))\n    \n    #Embeddings\n    input_embedding  = Embedding(nb_token=NB_TOKEN)(input_token)\n    output_embedding = Embedding(nb_token=NB_TOKEN_OUT)(output_token)\n    \n    #Add positional encoding\n    input_embedding  = input_embedding  + input_pos\n    output_embedding = output_embedding + output_pos\n    \n    #Encoder\n    encoder_output = Encoder(nb_encoder=NB_ENCODER)(input_embedding)\n    \n    #Mask\n    mask = tf.sequence_mask(tf.range(SEQ_LEN_OUT) + 1, SEQ_LEN_OUT)\n    mask = tf.cast(mask, tf.float32)\n    mask = tf.expand_dims(mask, axis=0)\n    \n    #Decoder\n    decoder_output = Decoder(NB_DECODER)((encoder_output, output_embedding, mask))\n    \n    #Predictions\n    output_prediction = tf.keras.layers.Dense(len(output_voc))(decoder_output)\n    predictions = tf.nn.softmax(output_prediction, axis=-1)\n    \n    model = tf.keras.Model([input_token, output_token], predictions)\n    model.summary()\n    return model\n    \ntransformer = build_transformer()\noutput = transformer((input_seq, output_seq))\nprint(output.shape)\nprint(output)","metadata":{"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Model: \"model_6\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_7 (InputLayer)            [(None, 5)]          0                                            \n__________________________________________________________________________________________________\nembedding_10 (Embedding)        (None, 5, 256)       1280        input_7[0][0]                    \n__________________________________________________________________________________________________\ninput_8 (InputLayer)            [(None, 6)]          0                                            \n__________________________________________________________________________________________________\ntf.__operators__.add (TFOpLambd (None, 5, 256)       0           embedding_10[0][0]               \n__________________________________________________________________________________________________\nembedding_11 (Embedding)        (None, 6, 256)       1536        input_8[0][0]                    \n__________________________________________________________________________________________________\nencoder_1 (Encoder)             (None, 5, 256)       1976832     tf.__operators__.add[0][0]       \n__________________________________________________________________________________________________\ntf.__operators__.add_1 (TFOpLam (None, 6, 256)       0           embedding_11[0][0]               \n__________________________________________________________________________________________________\ndecoder (Decoder)               (None, 6, 256)       1976832     encoder_1[0][0]                  \n                                                                 tf.__operators__.add_1[0][0]     \n__________________________________________________________________________________________________\ndense (Dense)                   (None, 6, 8)         2056        decoder[0][0]                    \n__________________________________________________________________________________________________\ntf.nn.softmax (TFOpLambda)      (None, 6, 8)         0           dense[0][0]                      \n==================================================================================================\nTotal params: 3,958,536\nTrainable params: 3,958,536\nNon-trainable params: 0\n__________________________________________________________________________________________________\n(1, 6, 8)\ntf.Tensor(\n[[[6.77362084e-02 1.08394085e-03 3.19027901e-02 6.06901804e-03\n   8.57202649e-01 1.80455705e-03 7.46860448e-03 2.67322417e-02]\n  [9.17770416e-02 7.25877762e-04 3.11953072e-02 6.00916799e-03\n   8.36304963e-01 2.02104961e-03 1.02350581e-02 2.17315014e-02]\n  [9.44109187e-02 1.20533945e-03 4.57033180e-02 6.30864734e-03\n   8.10230255e-01 2.60928529e-03 1.02499835e-02 2.92822961e-02]\n  [1.27809882e-01 2.35765963e-03 3.65079045e-02 9.72445123e-03\n   7.73350656e-01 4.00632014e-03 1.28335338e-02 3.34095843e-02]\n  [2.32651696e-01 1.31361256e-03 3.77439596e-02 1.02756200e-02\n   6.64923251e-01 2.54257326e-03 8.80997349e-03 4.17392887e-02]\n  [1.60265088e-01 2.75521772e-03 4.82685678e-02 1.13779083e-02\n   7.17513263e-01 6.74569234e-03 9.71357897e-03 4.33607101e-02]]], shape=(1, 6, 8), dtype=float32)\n","output_type":"stream"}]}]}